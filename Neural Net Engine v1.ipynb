{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# imports and small helper functions\nimport math\nimport sys\nfrom typing import Dict, Tuple, List\n\n# parse weight input string; treat 'na' (any case) as missing connection\ndef parse_weight_input(s: str):\n    s = s.strip()\n    if s.lower() in (\"na\", \"n/a\", \"\"):\n        return None\n    try:\n        return float(s)\n    except:\n        return None\n\n# prompt helpers for integers and floats with optional defaults\ndef ask_int(prompt: str, default=None):\n    while True:\n        try:\n            v = input(prompt).strip()\n            if v == \"\" and default is not None:\n                return default\n            iv = int(v)\n            return iv\n        except Exception:\n            print(\"Please enter an integer.\")\n\ndef ask_float(prompt: str, default=None):\n    while True:\n        v = input(prompt).strip()\n        if v == \"\" and default is not None:\n            return default\n        try:\n            return float(v)\n        except:\n            print(\"Please enter a number.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:14:37.482027Z","iopub.execute_input":"2025-12-01T14:14:37.482881Z","iopub.status.idle":"2025-12-01T14:14:37.493644Z","shell.execute_reply.started":"2025-12-01T14:14:37.482852Z","shell.execute_reply":"2025-12-01T14:14:37.492630Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Node class implementing scalar values and local backward rules\nclass Node:\n    \n    def __init__(self, out: float, prev=()):\n        self.out = float(out)\n        self.delta = 0.0\n        self._prev = tuple(prev)\n        self._backward = lambda: None\n        self.label = \"\"\n\n    def __repr__(self):\n        return f\"Node(label={self.label!r}, out={self.out:.6f}, delta={self.delta:.6f})\"\n\n    # Addition: z = a + b\n    def __add__(self, other):\n        other = other if isinstance(other, Node) else Node(float(other))\n        out = Node(self.out + other.out, (self, other))\n        def _backward():\n            # ∂(a+b)/∂a = 1, ∂(a+b)/∂b = 1\n            self.delta += 1.0 * out.delta\n            other.delta += 1.0 * out.delta\n        out._backward = _backward\n        return out\n\n    def __radd__(self, other):\n        return self + other\n\n    # Multiplication: z = a * b\n    def __mul__(self, other):\n        other = other if isinstance(other, Node) else Node(float(other))\n        out = Node(self.out * other.out, (self, other))\n        def _backward():\n            # ∂(a*b)/∂a = b ; ∂(a*b)/∂b = a\n            self.delta += other.out * out.delta\n            other.delta += self.out * out.delta\n        out._backward = _backward\n        return out\n\n    def __rmul__(self, other):\n        return self * other\n\n    # Negation and subtraction helpers\n    def __neg__(self):\n        out = Node(-self.out, (self,))\n        def _backward():\n            self.delta += -1.0 * out.delta\n        out._backward = _backward\n        return out\n\n    def __sub__(self, other):\n        other = other if isinstance(other, Node) else Node(float(other))\n        return self + (-other)\n\n    def __rsub__(self, other):\n        other = other if isinstance(other, Node) else Node(float(other))\n        return other - self\n\n    # Power (integer exponent) support, used for squared error\n    def __pow__(self, exponent: int):\n        out = Node(self.out ** exponent, (self,))\n        def _backward():\n            if exponent != 0:\n                self.delta += exponent * (self.out ** (exponent - 1)) * out.delta\n        out._backward = _backward\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:14:37.496012Z","iopub.execute_input":"2025-12-01T14:14:37.496385Z","iopub.status.idle":"2025-12-01T14:14:37.520381Z","shell.execute_reply.started":"2025-12-01T14:14:37.496361Z","shell.execute_reply":"2025-12-01T14:14:37.519303Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# activation functions and backward/topological traversal\n\ndef sigmoid(node: Node, lam: float = 1.0):\n    \n    #Sigmoid activation with scale parameter lambda:\n    #O = sigmoid(lambda * n) = 1 / (1 + exp(-lambda * n))\n    #Local derivative: dO/dn = lambda * O * (1 - O)\n    \n    s = 1.0 / (1.0 + math.exp(-lam * node.out))\n    out = Node(s, (node,))\n    def _backward():\n        node.delta += lam * (out.out * (1.0 - out.out)) * out.delta\n    out._backward = _backward\n    return out\n\ndef tanh_node(node: Node):\n    \n    #tanh activation and its derivative:\n    #O = tanh(n), dO/dn = 1 - tanh(n)^2\n    \n    t = math.tanh(node.out)\n    out = Node(t, (node,))\n    def _backward():\n        node.delta += (1.0 - out.out * out.out) * out.delta\n    out._backward = _backward\n    return out\n\ndef linear(node: Node):\n    \n    # Identity activation: O = n ; derivative = 1\n    \n    out = Node(node.out, (node,))\n    def _backward():\n        node.delta += 1.0 * out.delta\n    out._backward = _backward\n    return out\n\ndef build_topo(root: Node):\n    \n    visited = set()\n    topo = []\n    def dfs(n):\n        if id(n) in visited:\n            return\n        visited.add(id(n))\n        for p in n._prev:\n            dfs(p)\n        topo.append(n)\n    dfs(root)\n    return topo\n\ndef backward(root: Node):\n    \n    # Perform backward pass starting from root (the final scalar loss).\n    # Sets root.delta = 1.0 and calls each node's local _backward in reverse topo order.\n    root.delta = 1.0\n    topo = build_topo(root)\n    for n in reversed(topo):\n        n._backward()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:14:37.521857Z","iopub.execute_input":"2025-12-01T14:14:37.522279Z","iopub.status.idle":"2025-12-01T14:14:37.546048Z","shell.execute_reply.started":"2025-12-01T14:14:37.522244Z","shell.execute_reply":"2025-12-01T14:14:37.545120Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# network collection, graph build, forward, backprop, updates, and Graphviz\n\ndef collect_network_for_forward(allow_weights_input=True):\n\n    print(\"Choose activation for all hidden/output neurons: sigmoid / tanh / linear\")\n    act_choice = input(\"Activation (default sigmoid): \").strip().lower() or \"sigmoid\"\n    lam = 1.0\n    if act_choice == \"sigmoid\":\n        lam = ask_float(\"Enter sigmoid lambda (λ) (default 1.0): \", default=1.0)\n\n    in_count = ask_int(\"Number of input neurons (layer 1) (e.g. 2): \")\n    inputs = []\n    for i in range(in_count):\n        val = ask_float(f\"Value for input neuron {i}: \")\n        inputs.append(val)\n\n    hidden_layers = ask_int(\"Number of hidden layers (0 allowed): \")\n    hidden_counts = []\n    for h in range(hidden_layers):\n        c = ask_int(f\"Number of neurons in hidden layer {h+1}: \")\n        hidden_counts.append(c)\n\n    out_count = ask_int(\"Number of output neurons (final layer) (e.g. 1): \")\n\n    layer_sizes = [in_count] + hidden_counts + [out_count]\n    total_layers = len(layer_sizes)\n\n    weights = {}\n    print(\"\\nEnter connection weights between consecutive layers.\")\n    print(\"If a connection does not exist, enter NA (or na).\")\n    for L in range(total_layers - 1):\n        n_from = layer_sizes[L]\n        n_to = layer_sizes[L+1]\n        print(f\"\\nConnections from layer {L} (size {n_from}) to layer {L+1} (size {n_to}).\")\n        for i in range(n_from):\n            for j in range(n_to):\n                if allow_weights_input:\n                    s = input(f\"Weight from layer{L}_neuron{i} -> layer{L+1}_neuron{j}: \").strip()\n                    w = parse_weight_input(s)\n                else:\n                    w = None\n                if w is None:\n                    pass\n                else:\n                    weights[(L, i, j)] = float(w)\n\n    return {\n        \"activation\": act_choice,\n        \"lambda\": lam,\n        \"layer_sizes\": layer_sizes,\n        \"inputs\": inputs,\n        \"weights\": weights\n    }\n\ndef build_graph(params, make_weights_nodes=True):\n\n    act = params[\"activation\"]\n    lam = params[\"lambda\"]\n    sizes = params[\"layer_sizes\"]\n    inputs_vals = params[\"inputs\"]\n    raw_weights = params[\"weights\"]\n\n    input_nodes = [Node(v) for v in inputs_vals]\n    for idx, n in enumerate(input_nodes):\n        n.label = f\"in{idx}\"\n\n    weight_nodes = {}\n    layer_nodes = [input_nodes]\n\n    total_layers = len(sizes)\n    for L in range(1, total_layers):\n        n_to = sizes[L]\n        prev_nodes = layer_nodes[L-1]\n        act_nodes = []\n        for j in range(n_to):\n            sum_node = None\n            connected = False\n            for i, inp in enumerate(prev_nodes):\n                key = (L-1, i, j)\n                if key in raw_weights:\n                    connected = True\n                    if make_weights_nodes:\n                        if key not in weight_nodes:\n                            wnode = Node(raw_weights[key])\n                            wnode.label = f\"w_{L-1}_{i}_{j}\"\n                            weight_nodes[key] = wnode\n                        w = weight_nodes[key]\n                    else:\n                        w = Node(raw_weights[key])\n                    term = inp * w\n                    if sum_node is None:\n                        sum_node = term\n                    else:\n                        sum_node = sum_node + term\n            if sum_node is None:\n                sum_node = Node(0.0)\n            sum_node.label = f\"n_{L}_{j}\"\n            if act == \"sigmoid\":\n                out_node = sigmoid(sum_node, lam=lam)\n            elif act == \"tanh\":\n                out_node = tanh_node(sum_node)\n            else:\n                out_node = linear(sum_node)\n            out_node.label = f\"O_{L}_{j}\"\n            act_nodes.append(out_node)\n        layer_nodes.append(act_nodes)\n    return input_nodes, layer_nodes, weight_nodes\n\ndef run_forward(params, build_weights=True):\n    \n    in_nodes, layers, weight_nodes = build_graph(params, make_weights_nodes=build_weights)\n    outputs = layers[-1]\n    return in_nodes, layers, weight_nodes, outputs\n\ndef run_backprop(params, targets, eta):\n\n    in_nodes, layers, weight_nodes, outputs = run_forward(params, build_weights=True)\n    loss_node = None\n    for t, o in zip(targets, outputs):\n        tn = Node(t); tn.label = \"T\"\n        diff = tn - o\n        sq = 0.5 * (diff * diff)\n        if loss_node is None:\n            loss_node = sq\n        else:\n            loss_node = loss_node + sq\n    loss_node.label = \"L\"\n    backward(loss_node)\n    updated_weights = {}\n    for key, wnode in weight_nodes.items():\n        grad = wnode.delta\n        old = wnode.out\n        wnode.out = old - eta * grad\n        updated_weights[key] = (old, wnode.out, grad)\n    return loss_node, updated_weights, in_nodes, layers, outputs, weight_nodes\n\ndef show_graphviz(params, weight_nodes, layers):\n\n    try:\n        from graphviz import Digraph, Source\n    except Exception:\n        print(\"Neural Network cannot be shown because Graphviz is not properly installed\")\n        return\n    dot = Digraph(comment=\"Neural Network\")\n    total_layers = len(params[\"layer_sizes\"])\n    for L in range(total_layers):\n        for idx in range(params[\"layer_sizes\"][L]):\n            nid = f\"n_{L}_{idx}\"\n            label = f\"Layer{L}\\\\nNeuron {idx}\"\n            dot.node(nid, label=label, shape=\"circle\")\n    for (L,i,j), w in params[\"weights\"].items():\n        if (L,i,j) in weight_nodes:\n            wval = weight_nodes[(L,i,j)].out\n        else:\n            wval = params[\"weights\"][(L,i,j)]\n        u = f\"n_{L}_{i}\"\n        v = f\"n_{L+1}_{j}\"\n        dot.edge(u, v, label=f\"{wval:.4f}\")\n    try:\n        src = Source(dot.source)\n        display(src)  # works in Jupyter/Kaggle; if fails, fallback will catch it\n    except Exception:\n        print(\"Graphviz available but could not display inline. DOT source below:\")\n        print(dot.source)\n\n# Main interactive flow\ndef main():\n    print(\"Neural Network Calculator\")\n    print(\"Options:\")\n    print(\"A : Forward pass (no weight update)\")\n    print(\"B : Backpropagation (compute gradients and update weights; optionally run forward with updated weights)\")\n    choice = input(\"Enter A or B: \").strip().upper()\n    if choice not in (\"A\", \"B\"):\n        print(\"Invalid choice. Exiting.\")\n        return\n\n    params = collect_network_for_forward(allow_weights_input=True)\n\n    if choice == \"A\":\n        in_nodes, layers, weight_nodes, outputs = run_forward(params, build_weights=True)\n        print(\"\\nForward pass outputs:\")\n        for idx, o in enumerate(outputs):\n            print(f\"Output neuron {idx} : {o.out:.6f}\")\n        show_graphviz(params, weight_nodes, layers)\n        return\n\n    out_count = params[\"layer_sizes\"][-1]\n    targets = []\n    print(f\"Enter {out_count} target value(s) for output layer:\")\n    for j in range(out_count):\n        t = ask_float(f\"Target for output neuron {j}: \")\n        targets.append(t)\n    eta = ask_float(\"Enter learning rate η (e.g. 0.01): \", default=0.01)\n\n    loss_node, updated_weights, in_nodes, layers, outputs, weight_nodes = run_backprop(params, targets, eta)\n    print(\"\\nBackpropagation complete.\")\n    print(f\"Loss (before update): {loss_node.out:.10f}\")\n    print(\"\\nUpdated weights (old -> new) and gradients:\")\n    if not updated_weights:\n        print(\"No connections were provided, so no weights updated.\")\n    else:\n        for key,(old,new,grad) in updated_weights.items():\n            L,i,j = key\n            print(f\"Weight layer{L}_n{i} -> layer{L+1}_n{j} : {old:.6f} -> {new:.6f} (grad={grad:.6e})\")\n\n    run_post = input(\"\\nRun forward pass with updated weights? (Y/n) \").strip().lower() or \"y\"\n    if run_post[0] == \"y\":\n        new_weights = {}\n        for k,v in params[\"weights\"].items():\n            if k in weight_nodes:\n                new_weights[k] = weight_nodes[k].out\n            else:\n                new_weights[k] = v\n        params2 = {\n            \"activation\": params[\"activation\"],\n            \"lambda\": params[\"lambda\"],\n            \"layer_sizes\": params[\"layer_sizes\"],\n            \"inputs\": params[\"inputs\"],\n            \"weights\": new_weights\n        }\n        _, layers2, _, outputs2 = run_forward(params2, build_weights=False)\n        print(\"\\nForward outputs after weight update:\")\n        for idx,o in enumerate(outputs2):\n            print(f\"Output neuron {idx} : {o.out:.6f}\")\n        show_graphviz(params2, weight_nodes, layers2)\n    else:\n        show_graphviz(params, weight_nodes, layers)\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:14:37.600393Z","iopub.execute_input":"2025-12-01T14:14:37.600761Z","iopub.status.idle":"2025-12-01T14:16:28.757316Z","shell.execute_reply.started":"2025-12-01T14:14:37.600737Z","shell.execute_reply":"2025-12-01T14:16:28.756257Z"}},"outputs":[{"name":"stdout","text":"Neural Network Calculator\nOptions:\nA : Forward pass (no weight update)\nB : Backpropagation (compute gradients and update weights; optionally run forward with updated weights)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter A or B:  A\n"},{"name":"stdout","text":"Choose activation for all hidden/output neurons: sigmoid / tanh / linear\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Activation (default sigmoid):  sigmoid\nEnter sigmoid lambda (λ) (default 1.0):  1\nNumber of input neurons (layer 1) (e.g. 2):  2\nValue for input neuron 0:  0.35\nValue for input neuron 1:  0.9\nNumber of hidden layers (0 allowed):  1\nNumber of neurons in hidden layer 1:  2\nNumber of output neurons (final layer) (e.g. 1):  1\n"},{"name":"stdout","text":"\nEnter connection weights between consecutive layers.\nIf a connection does not exist, enter NA (or na).\n\nConnections from layer 0 (size 2) to layer 1 (size 2).\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Weight from layer0_neuron0 -> layer1_neuron0:  0.1\nWeight from layer0_neuron0 -> layer1_neuron1:  0.4\nWeight from layer0_neuron1 -> layer1_neuron0:  0.8\nWeight from layer0_neuron1 -> layer1_neuron1:  0.6\n"},{"name":"stdout","text":"\nConnections from layer 1 (size 2) to layer 2 (size 1).\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Weight from layer1_neuron0 -> layer2_neuron0:  0.3\nWeight from layer1_neuron1 -> layer2_neuron0:  0.9\n"},{"name":"stdout","text":"\nForward pass outputs:\nOutput neuron 0 : 0.690283\n","output_type":"stream"},{"output_type":"display_data","data":{"image/svg+xml":"<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"223pt\" height=\"398pt\"\n viewBox=\"0.00 0.00 223.08 398.50\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 394.5)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-394.5 219.08,-394.5 219.08,4 -4,4\"/>\n<!-- n_0_0 -->\n<g id=\"node1\" class=\"node\">\n<title>n_0_0</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"48.08\" cy=\"-342.42\" rx=\"48.17\" ry=\"48.17\"/>\n<text text-anchor=\"middle\" x=\"48.08\" y=\"-346.22\" font-family=\"Times,serif\" font-size=\"14.00\">Layer0</text>\n<text text-anchor=\"middle\" x=\"48.08\" y=\"-331.22\" font-family=\"Times,serif\" font-size=\"14.00\">Neuron 0</text>\n</g>\n<!-- n_1_0 -->\n<g id=\"node3\" class=\"node\">\n<title>n_1_0</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"50.08\" cy=\"-195.25\" rx=\"48.17\" ry=\"48.17\"/>\n<text text-anchor=\"middle\" x=\"50.08\" y=\"-199.05\" font-family=\"Times,serif\" font-size=\"14.00\">Layer1</text>\n<text text-anchor=\"middle\" x=\"50.08\" y=\"-184.05\" font-family=\"Times,serif\" font-size=\"14.00\">Neuron 0</text>\n</g>\n<!-- n_0_0&#45;&gt;n_1_0 -->\n<g id=\"edge1\" class=\"edge\">\n<title>n_0_0&#45;&gt;n_1_0</title>\n<path fill=\"none\" stroke=\"black\" d=\"M22.29,-301.35C17,-288.77 13.89,-274.67 17.08,-261.33 18.27,-256.38 19.89,-251.37 21.8,-246.46\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"25.06,-247.72 25.74,-237.15 18.62,-244.99 25.06,-247.72\"/>\n<text text-anchor=\"middle\" x=\"36.08\" y=\"-265.13\" font-family=\"Times,serif\" font-size=\"14.00\">0.1000</text>\n</g>\n<!-- n_1_1 -->\n<g id=\"node4\" class=\"node\">\n<title>n_1_1</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"164.08\" cy=\"-195.25\" rx=\"48.17\" ry=\"48.17\"/>\n<text text-anchor=\"middle\" x=\"164.08\" y=\"-199.05\" font-family=\"Times,serif\" font-size=\"14.00\">Layer1</text>\n<text text-anchor=\"middle\" x=\"164.08\" y=\"-184.05\" font-family=\"Times,serif\" font-size=\"14.00\">Neuron 1</text>\n</g>\n<!-- n_0_0&#45;&gt;n_1_1 -->\n<g id=\"edge2\" class=\"edge\">\n<title>n_0_0&#45;&gt;n_1_1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M59.49,-295.44C63.94,-283.48 69.99,-271.21 78.08,-261.33 87.7,-249.6 94.76,-252.18 107.08,-243.33 111.28,-240.32 115.56,-237.09 119.8,-233.79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"122.21,-236.35 127.87,-227.39 117.86,-230.87 122.21,-236.35\"/>\n<text text-anchor=\"middle\" x=\"97.08\" y=\"-265.13\" font-family=\"Times,serif\" font-size=\"14.00\">0.4000</text>\n</g>\n<!-- n_0_1 -->\n<g id=\"node2\" class=\"node\">\n<title>n_0_1</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"166.08\" cy=\"-342.42\" rx=\"48.17\" ry=\"48.17\"/>\n<text text-anchor=\"middle\" x=\"166.08\" y=\"-346.22\" font-family=\"Times,serif\" font-size=\"14.00\">Layer0</text>\n<text text-anchor=\"middle\" x=\"166.08\" y=\"-331.22\" font-family=\"Times,serif\" font-size=\"14.00\">Neuron 1</text>\n</g>\n<!-- n_0_1&#45;&gt;n_1_0 -->\n<g id=\"edge3\" class=\"edge\">\n<title>n_0_1&#45;&gt;n_1_0</title>\n<path fill=\"none\" stroke=\"black\" d=\"M142.93,-300.24C135.01,-287.35 125.71,-273.36 116.08,-261.33 109,-252.49 100.83,-243.59 92.69,-235.34\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"95.14,-232.84 85.59,-228.26 90.2,-237.8 95.14,-232.84\"/>\n<text text-anchor=\"middle\" x=\"145.08\" y=\"-265.13\" font-family=\"Times,serif\" font-size=\"14.00\">0.8000</text>\n</g>\n<!-- n_0_1&#45;&gt;n_1_1 -->\n<g id=\"edge4\" class=\"edge\">\n<title>n_0_1&#45;&gt;n_1_1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M176.15,-295.33C177.54,-284.22 178.17,-272.37 177.08,-261.33 176.8,-258.43 176.44,-255.47 176.03,-252.48\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"179.44,-251.65 174.45,-242.3 172.52,-252.72 179.44,-251.65\"/>\n<text text-anchor=\"middle\" x=\"196.08\" y=\"-265.13\" font-family=\"Times,serif\" font-size=\"14.00\">0.6000</text>\n</g>\n<!-- n_2_0 -->\n<g id=\"node5\" class=\"node\">\n<title>n_2_0</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"106.08\" cy=\"-48.08\" rx=\"48.17\" ry=\"48.17\"/>\n<text text-anchor=\"middle\" x=\"106.08\" y=\"-51.88\" font-family=\"Times,serif\" font-size=\"14.00\">Layer2</text>\n<text text-anchor=\"middle\" x=\"106.08\" y=\"-36.88\" font-family=\"Times,serif\" font-size=\"14.00\">Neuron 0</text>\n</g>\n<!-- n_1_0&#45;&gt;n_2_0 -->\n<g id=\"edge5\" class=\"edge\">\n<title>n_1_0&#45;&gt;n_2_0</title>\n<path fill=\"none\" stroke=\"black\" d=\"M67.19,-149.91C72.94,-134.99 79.42,-118.19 85.42,-102.65\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"88.75,-103.74 89.08,-93.16 82.22,-101.23 88.75,-103.74\"/>\n<text text-anchor=\"middle\" x=\"98.08\" y=\"-117.97\" font-family=\"Times,serif\" font-size=\"14.00\">0.3000</text>\n</g>\n<!-- n_1_1&#45;&gt;n_2_0 -->\n<g id=\"edge6\" class=\"edge\">\n<title>n_1_1&#45;&gt;n_2_0</title>\n<path fill=\"none\" stroke=\"black\" d=\"M146.52,-150.3C140.5,-135.24 133.7,-118.21 127.42,-102.48\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"130.54,-100.87 123.58,-92.88 124.04,-103.47 130.54,-100.87\"/>\n<text text-anchor=\"middle\" x=\"155.08\" y=\"-117.97\" font-family=\"Times,serif\" font-size=\"14.00\">0.9000</text>\n</g>\n</g>\n</svg>\n","text/plain":"<graphviz.sources.Source at 0x7e34e8ee9910>"},"metadata":{}}],"execution_count":4}]}